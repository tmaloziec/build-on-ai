#!/bin/bash
# Bielik AI Integration dla BuildOnAI - Polski AI Assistant

echo "ğŸ¦… Dodawanie Bielik AI do BuildOnAI"
echo "=================================="

# Bielik jest dostÄ™pny przez rÃ³Å¼ne platformy
install_bielik_options() {
    echo "ğŸ‡µğŸ‡± Opcje instalacji Bielik AI:"
    echo ""
    echo "1. Bielik przez Ollama (zalecane)"
    echo "2. Bielik przez HuggingFace Transformers"
    echo "3. Bielik API (jeÅ›li dostÄ™pne)"
    echo "4. Wszystkie opcje"
}

# Opcja 1: Bielik przez Ollama
install_bielik_ollama() {
    echo "ğŸ“¦ Instalowanie Bielik przez Ollama..."
    
    # SprawdÅº czy Bielik jest dostÄ™pny w Ollama
    if ollama list | grep -q "bielik"; then
        echo "âœ… Bielik juÅ¼ zainstalowany!"
        return 0
    fi
    
    # SprÃ³buj pobraÄ‡ Bielik
    echo "ğŸ§  Pobieranie Bielik AI model..."
    
    # RÃ³Å¼ne wersje Bielik do sprawdzenia
    BIELIK_MODELS=(
        "bielik-7b"
        "bielik-7b-instruct"
        "bielik-13b"
        "bielik"
        "speakleash/bielik-7b-instruct-v0.1"
    )
    
    for model in "${BIELIK_MODELS[@]}"; do
        echo "ğŸ” PrÃ³ba instalacji: $model"
        if timeout 60s ollama pull "$model" 2>/dev/null; then
            echo "âœ… Bielik zainstalowany jako: $model"
            echo "$model" > ~/.config/build-on-ai/bielik-model
            return 0
        fi
    done
    
    echo "âš ï¸  Bielik nie jest dostÄ™pny w standardowym Ollama"
    echo "ğŸ’¡ SprÃ³bujemy alternatywnych metod..."
    return 1
}

# Opcja 2: Bielik przez HuggingFace
install_bielik_transformers() {
    echo "ğŸ¤— Instalowanie Bielik przez HuggingFace Transformers..."
    
    # Instaluj wymagane biblioteki
    pip3 install torch transformers accelerate sentencepiece
    
    # UtwÃ³rz Bielik script
    cat > ~/build-on-ai/bielik-hf.py << 'EOF'
#!/usr/bin/env python3
"""
Bielik AI przez HuggingFace Transformers
"""

import sys
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class BielikAI:
    def __init__(self):
        self.model_name = "speakleash/bielik-7b-instruct-v0.1"
        self.tokenizer = None
        self.model = None
        self.loaded = False
        
    def load_model(self):
        """ZaÅ‚aduj model Bielik"""
        if self.loaded:
            return True
            
        try:
            print("ğŸ¦… Åadowanie Bielik AI model...")
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True
            )
            
            self.loaded = True
            print("âœ… Bielik AI zaÅ‚adowany!")
            return True
            
        except Exception as e:
            print(f"âŒ BÅ‚Ä…d Å‚adowania Bielik: {e}")
            return False
    
    def ask(self, question):
        """Zadaj pytanie Bielik AI"""
        if not self.load_model():
            return "âŒ Nie udaÅ‚o siÄ™ zaÅ‚adowaÄ‡ Bielik AI"
            
        try:
            # Format prompt dla Bielik
            prompt = f"### Instrukcja:\nJesteÅ› pomocnym asystentem AI dla inÅ¼ynierÃ³w pracujÄ…cych z Linux. Odpowiadaj praktycznie i zwiÄ™Åºle.\n\n### Pytanie:\n{question}\n\n### OdpowiedÅº:\n"
            
            inputs = self.tokenizer(prompt, return_tensors="pt").to(self.model.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # WyciÄ…gnij odpowiedÅº (po "### OdpowiedÅº:")
            if "### OdpowiedÅº:" in response:
                answer = response.split("### OdpowiedÅº:")[-1].strip()
                return answer
            else:
                return response[len(prompt):].strip()
                
        except Exception as e:
            return f"âŒ BÅ‚Ä…d generowania odpowiedzi: {e}"

def main():
    if len(sys.argv) < 2:
        print("ğŸ¦… Bielik AI - Polski Assistant")
        print("Usage: python3 bielik-hf.py 'twoje pytanie'")
        sys.exit(1)
        
    question = " ".join(sys.argv[1:])
    
    bielik = BielikAI()
    response = bielik.ask(question)
    
    print("\nğŸ¦… Bielik AI:")
    print("=" * 50)
    print(response)
    print("=" * 50)

if __name__ == "__main__":
    main()
EOF

    chmod +x ~/build-on-ai/bielik-hf.py
    echo "âœ… Bielik HuggingFace script utworzony!"
}

# Opcja 3: SprawdÅº Bielik API
check_bielik_api() {
    echo "ğŸŒ Sprawdzanie dostÄ™pnoÅ›ci Bielik API..."
    
    # MoÅ¼liwe endpointy Bielik API
    BIELIK_ENDPOINTS=(
        "https://api.bielik.ai"
        "https://bielik-api.pl"
        "https://api.speakleash.org"
    )
    
    for endpoint in "${BIELIK_ENDPOINTS[@]}"; do
        echo "ğŸ” Sprawdzanie: $endpoint"
        if curl -s --max-time 5 "$endpoint" >/dev/null 2>&1; then
            echo "âœ… Znaleziono API: $endpoint"
            echo "$endpoint" > ~/.config/build-on-ai/bielik-api-endpoint
            return 0
        fi
    done
    
    echo "âš ï¸  Brak dostÄ™pnych API endpoints"
    return 1
}

# StwÃ³rz polskie komendy AI
create_polish_ai_commands() {
    echo "ğŸ‡µğŸ‡± Tworzenie polskich komend AI..."
    
    # Komenda ai-pl (Bielik)
    sudo tee /usr/local/bin/ai-pl << 'EOF'
#!/bin/bash
# BuildOnAI - Polski AI Assistant (Bielik)

if [[ $# -eq 0 ]]; then
    echo "ğŸ¦… BuildOnAI - Polski AI Assistant (Bielik)"
    echo "Usage: ai-pl 'twoje pytanie po polsku'"
    echo ""
    echo "PrzykÅ‚ady:"
    echo "  ai-pl 'Jak zainstalowaÄ‡ Python w Ubuntu?'"
    echo "  ai-pl 'WyjaÅ›nij co to jest machine learning'"
    echo "  ai-pl 'PomÃ³Å¼ z debugowaniem skryptu bash'"
    exit 0
fi

QUESTION="$*"

# SprÃ³buj Bielik przez Ollama
if [[ -f ~/.config/build-on-ai/bielik-model ]]; then
    BIELIK_MODEL=$(cat ~/.config/build-on-ai/bielik-model)
    if ollama list | grep -q "$BIELIK_MODEL"; then
        echo "ğŸ¦… UÅ¼ywam Bielik przez Ollama..."
        ollama run "$BIELIK_MODEL" "JesteÅ› pomocnym asystentem AI dla polskich inÅ¼ynierÃ³w. Odpowiadaj po polsku, praktycznie i zwiÄ™Åºle. Pytanie: $QUESTION"
        exit 0
    fi
fi

# SprÃ³buj Bielik przez HuggingFace
if [[ -f ~/build-on-ai/bielik-hf.py ]]; then
    echo "ğŸ¦… UÅ¼ywam Bielik przez HuggingFace..."
    python3 ~/build-on-ai/bielik-hf.py "$QUESTION"
    exit 0
fi

# Fallback na Claude z polskim promptem
if [[ -n "$CLAUDE_API_KEY" ]]; then
    echo "ğŸ¦… Fallback na Claude z polskim promptem..."
    ai-cloud "Odpowiedz po polsku na to pytanie: $QUESTION"
    exit 0
fi

# Ostateczny fallback na lokalny model z polskim promptem
echo "ğŸ¦… UÅ¼ywam lokalnego AI z polskim promptem..."
ai-local "Answer in Polish language: $QUESTION"
EOF

    sudo chmod +x /usr/local/bin/ai-pl
    
    # Dodaj aliasy polskie
    cat >> ~/.bashrc << 'EOF'

# BuildOnAI - Polskie aliasy AI
alias pytaj='ai-pl'
alias pomoc='ai-pl'
alias asystent='ai-pl'
EOF

    echo "âœ… Polskie komendy utworzone!"
    echo "DostÄ™pne: ai-pl, pytaj, pomoc, asystent"
}

# Update smart model selector dla jÄ™zyka polskiego
update_model_selector_polish() {
    echo "ğŸ‡µğŸ‡± Aktualizowanie selektora dla jÄ™zyka polskiego..."
    
    # Backup oryginalnego ai-select
    if [[ -f /usr/local/bin/ai-select ]]; then
        sudo cp /usr/local/bin/ai-select /usr/local/bin/ai-select.backup
    fi
    
    # Dodaj detekcjÄ™ jÄ™zyka polskiego
    sudo tee -a /usr/local/bin/ai-select << 'EOF'

# Detekcja jÄ™zyka polskiego
detect_polish() {
    local text="$1"
    # Polskie znaki i sÅ‚owa
    if [[ "$text" =~ [Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼Ä„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»] ]] || \
       [[ "$text" =~ (jak|co|gdzie|dlaczego|kiedy|czy|Å¼eby|bardzo|przez|oraz|tylko|takÅ¼e|moÅ¼e|naleÅ¼y) ]]; then
        return 0  # Polski
    fi
    return 1  # Nie polski
}

# JeÅ›li pytanie po polsku, uÅ¼yj Bielik
if detect_polish "$QUESTION"; then
    echo "ğŸ‡µğŸ‡± Wykryto jÄ™zyk polski - uÅ¼ywam Bielik AI..."
    ai-pl "$QUESTION"
    exit 0
fi
EOF

    echo "âœ… Selektor zaktualizowany dla jÄ™zyka polskiego!"
}

# Menu instalacji
show_bielik_menu() {
    echo ""
    echo "ğŸ¦… Menu instalacji Bielik AI:"
    echo "1. install_bielik_ollama        # Przez Ollama (zalecane)"
    echo "2. install_bielik_transformers  # Przez HuggingFace" 
    echo "3. check_bielik_api            # SprawdÅº API"
    echo "4. create_polish_ai_commands   # Polskie komendy"
    echo "5. update_model_selector_polish # Update selektor"
    echo "6. ALL - Zainstaluj wszystko"
    echo ""
    echo "ğŸ¯ Po instalacji:"
    echo "  ai-pl 'Jak zainstalowaÄ‡ Docker?'"
    echo "  pytaj 'Co to jest Kubernetes?'"
    echo "  pomoc 'Debugowanie bash script'"
}

# Funkcja instalujÄ…ca wszystko
install_all_bielik() {
    echo "ğŸš€ Instalowanie wszystkich opcji Bielik AI..."
    
    install_bielik_ollama
    install_bielik_transformers  
    check_bielik_api
    create_polish_ai_commands
    update_model_selector_polish
    
    echo "ğŸ‰ Bielik AI kompletnie zintegrowany!"
}

# Uruchom menu
install_bielik_options
show_bielik_menu
